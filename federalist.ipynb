{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import functools as ft\n",
    "from itertools import chain\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from yellowbrick.classifier import (ClassBalance, ClassificationReport,\n",
    "                                    ClassPredictionError, ConfusionMatrix)\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will provide a mean value embedding vectorizer.  That is, for a sentence, it will take each word\n",
    "# vectorize it and then take the mean of those vectors and return a single vector.\n",
    "# It defines \"fit\" and \"transform\" so it can fit into a python pipeline\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(w2v))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        retVal = []\n",
    "        for line in X:\n",
    "            temp = np.array(np.mean([self.word2vec[w] for w in line.split() if w in self.word2vec] \n",
    "                                or [np.zeros(self.dim)], axis=0))\n",
    "            retVal.append(temp)\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def openEmbeddings(file):\n",
    "    embeddings_index = {}\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def word2idx(word, word_model):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx, word_model):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelStats(model):\n",
    "    pred = model.predict(X_test_k)\n",
    "    classes = pred.round()# pred.argmax(axis=-1)\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_test_k, classes)))\n",
    "    print(classification_report(y_test_k, classes, digits=5))  \n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test_k, classes)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_seq_items = 100\n",
    "pd.options.display.max_columns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = 'federalist.csv'\n",
    "\n",
    "cols = ['number', 'author', 'text']\n",
    "target_col = 'author'\n",
    "data = pd.read_csv(TRAIN_SET_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.mask(data.eq('JAY')).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data['author'].value_counts(sort=True).index.tolist()\n",
    "majo = classes[0]\n",
    "mino = classes[1]\n",
    "data_maj = data[data[target_col] == majo]\n",
    "data_min = data[data[target_col] == mino]\n",
    "\n",
    "len(data_min), len(data_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_min = len(data_min)\n",
    "class_max = len(data_maj)\n",
    "class_min, class_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_diff = class_max - class_min \n",
    "desired_target = class_min + round(class_diff/2)\n",
    "\n",
    "# Upsample the minority class\n",
    "data_min_up = resample(data_min, \n",
    "                replace=True,     # sample with replacement\n",
    "                n_samples=desired_target,    \n",
    "                random_state=1) # reproducible results\n",
    "\n",
    "\n",
    "# Upsample the minority class\n",
    "data_maj_down = resample(data_maj, \n",
    "                    replace=True,     # sample with replacement\n",
    "                    n_samples=desired_target,    \n",
    "                    random_state=1) # reproducible results\n",
    "    \n",
    "# Combine upsampled minority class with downsampled majority class\n",
    "bal_set = pd.concat([data_min_up, data_maj_down])\n",
    " \n",
    "# Display new class counts\n",
    "bal_set.author.value_counts()\n",
    "\n",
    "# use our balanced set\n",
    "data = bal_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "lab = data[target_col]\n",
    "enc.fit(lab)\n",
    "\n",
    "y = pd.Series(enc.transform(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lables_index = dict(zip(lab.unique(), y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop([target_col], axis=1)\n",
    "data = data.drop(['number'], axis=1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.applymap(lambda x: x.lower() if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_glove_100 = openEmbeddings('glove.6B.100d.txt')\n",
    "embeddings_index_glove_300 = openEmbeddings('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "texts = data['text']\n",
    "\n",
    "# manual tokens, first we split on the periods\n",
    "sentences = [[sent for sent in d.split('<p>')] for d in texts]\n",
    "\n",
    "# combine our stentences into a list of lists to be vectorized later\n",
    "sentences_list = list(chain.from_iterable(sentences))\n",
    "\n",
    "# w2v training wants a list of list of tokens for each sentence\n",
    "sentences_train = [l.split() for l in sentences_list]\n",
    "\n",
    "# keras\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# this will give us a list of lists of our tokens\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "#get our unique words\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#this will reshape our list of list of tokens into a matrix of <entries> x <maxlen> and pad shorter ones\n",
    "X_k = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = [(TaggedDocument(simple_preprocess(line),[i])) for i,line in enumerate(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = model = Doc2Vec(vector_size=300, min_count=2, epochs=40)\n",
    "d2v_model.build_vocab(tagged_documents)\n",
    "d2v = {d: vec for d, vec in zip(d2v_model.wv.index2word, d2v_model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_doc = [d2v_model.docvecs[i] for i in range(len(d2v_model.docvecs))]\n",
    "X_doc = pd.DataFrame(data=X_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences_train, size=300, window=5, min_count=5, iter=100, workers=-1)\n",
    "ft_model = FastText(sentences_train, size=300, window=5, min_count=5, iter=100, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = {w: vec for w, vec in zip(w2v_model.wv.index2word, w2v_model.wv.syn0)}\n",
    "ftm = {w: vec for w, vec in zip(ft_model.wv.index2word, ft_model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v['union'].shape, ftm['union'].shape, embeddings_index_glove_100['union'].shape, embeddings_index_glove_300['union'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mean value embedding for each sentences here\n",
    "sentence_vectors = [MeanEmbeddingVectorizer(d2v).transform(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_vectors = [list(chain.from_iterable(vec)) for vec in sentence_vectors]\n",
    "flat_vectors.sort(key=len)\n",
    "shortest = len(flat_vectors[0])\n",
    "trimmed_flat_vectors = [l[:shortest] for l in flat_vectors]\n",
    "X_m = pd.DataFrame(data=trimmed_flat_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(sequences), X_k.shape, y.shape)\n",
    "print(y.unique().shape)\n",
    "X_m.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = embeddings_index_glove_300 #can be embeddings_index_glove_300, 100, w2v, ftm\n",
    "embedding_dim = len(next(iter(embeddings_index.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keras\n",
    "X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(X_k, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#manual\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_m, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#doc2vec\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_doc, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is assuming the use of the keras tokenizer to generate word_index\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index[word] if word in embeddings_index else None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 2, activation='relu')(embedded_sequences)\n",
    "x = Dropout(0.3)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(X_train_k, y_train_k,\n",
    "          batch_size=20,\n",
    "          epochs=50,\n",
    "          validation_data=(X_test_k, y_test_k),\n",
    "          callbacks=[PlotLossesKeras()])\n",
    "\n",
    "score_train = model.evaluate(X_train_k, y_train_k, batch_size=20, verbose=1)\n",
    "score_test = model.evaluate(X_test_k, y_test_k, batch_size=20, verbose=1)\n",
    "\n",
    "print(score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['union', 'constitution', 'life', 'liberty']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "                           for similar, dist in w2v_model.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index = w2v\n",
    "# embedding_matrix = np.zeros((MAX_NUM_WORDS, 100))\n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     if index > MAX_NUM_WORDS - 1:\n",
    "#         break\n",
    "#     else:\n",
    "        \n",
    "#         embedding_vector = embeddings_index.get(word) if (word in embeddings_index) else None\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(embedding_matrix), embedding_dim, input_length=1000, weights=[embedding_matrix], trainable=False))\n",
    "#model_glove.add(Dropout(0.2))\n",
    "#model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "#model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model_glove.fit(X_train_k, y_train_k,\n",
    "          batch_size=20,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test_k, y_test_k),\n",
    "          callbacks=[PlotLossesKeras()])\n",
    "\n",
    "score_train = model_glove.evaluate(X_train_k, y_train_k, batch_size=20, verbose=1)\n",
    "score_test = model_glove.evaluate(X_test_k, y_test_k, batch_size=20, verbose=1)\n",
    "\n",
    "print(score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelStats(model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=500, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_d, y_train_d)\n",
    "print(clf.score(X_train_d, y_train_d))\n",
    "print(clf.score(X_test_d, y_test_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ConfusionMatrix(clf, classes=y.unique())\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "for label in visualizer.ax.texts:\n",
    "    label.set_size(12)\n",
    "\n",
    "visualizer.fit(X_train_k, y_train_k)\n",
    "visualizer.score(X_test_k, y_test_k)\n",
    "\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
